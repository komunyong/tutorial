{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@enozeren/word2vec-from-scratch-with-python-1bba88d9f221\n",
    "https://medium.com/@anmoltalwar/cbow-word2vec-854a043ee8f3\n",
    "\n",
    "- One Hot Encodings\n",
    "- Co-occurance Matrix\n",
    "- Word Embeddings\n",
    "\n",
    "Steps Involved\n",
    "\n",
    "- Corpus Creation: Collect a large corpus of text data. This could be a collection of books, articles, or even social media posts.\n",
    "- Tokenization: Break the text down into individual words or tokens.\n",
    "- Vocabulary Building: Create a vocabulary of all unique words in the corpus.\n",
    "- Word Embedding Model Selection: Choose a suitable word embedding model. Popular options include:\n",
    "- Word2Vec (skip-gram or CBOW)\n",
    "    - GloVe\n",
    "    - FastText\n",
    "- Model Training: Train the selected model on the corpus. This involves iteratively adjusting the word embeddings to minimize a loss function.\n",
    "- Vector Lookup: Once trained, the model can be used to look up the vector representation of any word in the vocabulary.\n",
    "\n",
    "\n",
    "NOTE: In LLM will split one single corpus (rich text document) into sentence. The `batch` = 30, where `batch` is number of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encodings of each word in our Corpus\n",
    "- no semantic similarities between word vectors\n",
    "\n",
    "```python\n",
    "dogs   = [1, 0, 0, 0, 0, 0]\n",
    "and    = [0, 1, 0, 0, 0, 0]\n",
    "cats   = [0, 0, 1, 0, 0, 0]\n",
    "love   = [0, 0, 0, 1, 0, 0]\n",
    "this   = [0, 0, 0, 0, 1, 0]\n",
    "meadow = [0, 0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Co-occurance Matrix\n",
    "\n",
    "- Have space and memory consumed when have a big vocab\n",
    "\n",
    "<img src=\"images/word2vec_s1.png\">\n",
    "\n",
    "\n",
    "\n",
    "#### Word Embedding\n",
    "\n",
    "- compress the co-occurance matrix to a lower dimension with Singular Value Decomposition (SVD). This was called LSA (latent semantic analysis) in the literature.\n",
    "```python\n",
    "dogs   = [0.14, 3.42]\n",
    "and    = [0.90, 1.12]\n",
    "cats   = [2.30, -0.56]\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW\n",
    "\n",
    "CBOW is a neural network model used for word embeddings. It predicts a target word given its surrounding context words. It's one of the two main architectures used in Word2Vec.\n",
    "1. Word Embedding: one-hot-endoding or random initialized\n",
    "2. Define context window size and extract\n",
    "3. Aggregated context vector: avg or sum for all words in context window to prepare for Shallow NN\n",
    "4. Train through Shallow Neural Network\n",
    "6. Test model\n",
    "\n",
    "##### Example:\n",
    "\n",
    "- Corpus: \"The cat sat on the mat. The dog chased the cat.\"\n",
    "- Context window size: 2 (user's defined)\n",
    "- Vocabulary: {\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"chased\"}\n",
    "- Embedding dimension: 3 (user's defined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Embedding Initialization\n",
    "Let's randomly initialize the embeddings for each word:\n",
    "\n",
    "```json\n",
    "\"the\": [0.1, 0.2, 0.3]\n",
    "\"cat\": [0.4, 0.5, 0.6]\n",
    "\"sat\": [0.7, 0.8, 0.9]\n",
    "\"on\": [1.0, 1.1, 1.2]\n",
    "\"mat\": [1.3, 1.4, 1.5]\n",
    "\"dog\": [1.6, 1.7, 1.8]\n",
    "\"chased\": [1.9, 2.0, 2.1]\n",
    "```\n",
    "\n",
    "##### Step 2: Context Window and Embedding Lookup\n",
    "\n",
    "Target word: \"cat\"\n",
    "Context window: [\"the\", \"sat\"]\n",
    "\n",
    "2.A. Word Embeddings:\n",
    "```json\n",
    "\"the\": [0.1, 0.2, 0.3]\n",
    "\"sat\": [0.7, 0.8, 0.9]\n",
    "```\n",
    "\n",
    "can do OHC\n",
    "\n",
    "2.B One-Hot Encoding (OHC):\n",
    "```json\n",
    "\"the\": [1, 0, 0, 0, 0, 0, 0]\n",
    "\"sat\": [0, 0, 1, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "##### Step 3: Aggregated context vector (average or sum)\n",
    "to create a single vector representation of the context words.\n",
    "\n",
    "3.A Average the word embeddings:\n",
    "```json\n",
    "(0.1 + 0.7) / 2 = 0.4\n",
    "(0.2 + 0.8) / 2 = 0.5\n",
    "(0.3 + 0.9) / 2 = 0.6\n",
    "\n",
    "[0.4, 0.5, 0.6]\n",
    "```\n",
    "\n",
    "3.B Average OHC:\n",
    "```json\n",
    "(1 + 0) / 2 = 0.5\n",
    "(0 + 0) / 2 = 0\n",
    "(0 + 1) / 2 = 0.5\n",
    "(0 + 0) / 2 = 0\n",
    "(0 + 0) / 2 = 0\n",
    "(0 + 0) / 2 = 0\n",
    "(0 + 0) / 2 = 0\n",
    "\n",
    "[0.5, 0, 0.5, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "\n",
    "##### Step 4: Training with Shallow Neuron Network\n",
    "\n",
    "INPUT   : [0.4, 0.5, 0.6] (CBOW)\n",
    "          [w1, w2, w3] (weight initialize at first round)\n",
    "OUTPUT  : [o1] (predicted result from all vocabs)\n",
    "PROCESS : \n",
    "- get input embedding\n",
    "- initialize weight (or use adjusted weights)\n",
    "- dot product from input and weight \n",
    "- Softmax activation function\n",
    "- cross entropy to calculate loss\n",
    "- backpropagation (repeat with updated weight)\n",
    "\n",
    "\n",
    "\n",
    "##### Step 5: Testing new vocab with pre-trained Word2Vec\n",
    "INPUT: New word\n",
    "OUTPUT: Predicted Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of CBOW Input Calculation\n",
    "<b>Given</b>:\n",
    "\n",
    "\n",
    "\n",
    "Word Embeddings (input):\n",
    "|Word|Random Vector|\n",
    "|-|-|\n",
    "|cat|[0.1, 0.2, 0.3]|\n",
    "|dog|[0.4, 0.5, 0.6]|\n",
    "|bird|[0.7, 0.8, 0.9]|\n",
    "|tree|[0.1, 0.2, 0.3]|\n",
    "|flower|[0.4, 0.5, 0.6]|\n",
    "\n",
    "<br>\n",
    "\n",
    "|Parameters|Value|\n",
    "|-|-|\n",
    "|Vocabulary|`{cat, dog, bird, tree, flower}`|\n",
    "|Window Size|2|\n",
    "|Target Word|\"dog\"|\n",
    "|Input Weight Matrix (W_in)|[[0.2, 0.3], [0.4, 0.5], [0.6, 0.7]]|\n",
    "\n",
    "##### Steps:\n",
    "\n",
    "- Create a window: The window around \"dog\" would be [\"cat\", \"dog\", \"bird\"].\n",
    "- Represent words as vectors: The vectors for \"cat\", \"dog\", and \"bird\" are given.\n",
    "- Calculate the average word vector:\n",
    "- Average of the three vectors: ([0.1+0.4+0.7]/3, [0.2+0.5+0.8]/3, [0.3+0.6+0.9]/3) = [0.4, 0.5, 0.6].\n",
    "- Multiply by the input weights: [0.4, 0.5, 0.6] * [[0.2, 0.3], [0.4, 0.5], [0.6, 0.7]] = [0.56, 0.77].\n",
    "- Apply activation function: Assuming a sigmoid activation function:\n",
    "    - sigmoid(0.56) ≈ 0.63\n",
    "    - sigmoid(0.77) ≈ 0.68\n",
    "\n",
    "- Result: The input to the hidden layer for the word \"dog\" is approximately [0.63, 0.68]. These values will be used as inputs to the hidden layer neurons in the CBOW model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
