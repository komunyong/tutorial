{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "REF: https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6\n",
    "\n",
    "INPUT: Word Vector / Word Extracted from Corpus\n",
    "OUTPUT: Word Vector that added positional encoding\n",
    "\n",
    "- Periodicity (repeatability of their pattern)\n",
    "- easy to predict long sequence\n",
    "- constrained value\n",
    "    - Nearest word will get highest output and gradualy decrease by how far from first word.\n",
    "    - sin and cosine output in -1 to 1\n",
    "- Give position to each word in context to provide a relative position in word sequence\n",
    "- The model uses embedding vectors of length `d_model` to represent each word as embedding matrix\n",
    "- Use Sine and Cosine function to generate unique vector for each position in sequence\n",
    "    - Better than integer because Sine and Cosine output will scoped in [-1, 1]\n",
    "    - No additional training has to be done since unique representations are generated for each position.\n",
    "- Positional encoding matrix: will always produce the same output (and use to added back to input later)\n",
    "- This layer is just added (+) fixed set with non-learnable parameters\n",
    "- Parameter\n",
    "    - max_length >= len(input): to ensure to support future input length (L)\n",
    "    - n: recommend 10000 by paper\n",
    "    - d_model: dimensions of input ()\n",
    "\n",
    "<img src=\"./images/positional_encoding_s1.png\">\n",
    "\n",
    "#### Steps\n",
    "- Get input corpus\n",
    "- Extracted batch by window size\n",
    "- Replace with positional index [0, n]\n",
    "- Create positional matrix followed by equation\n",
    "- Do word embedding to input\n",
    "- Add word embedding with positional matrix\n",
    "\n",
    "\n",
    "\n",
    "##### Step 1:\n",
    "\n",
    "\n",
    "Q: Why use both sine and cosine to do unique\n",
    "\n",
    "A: To further reduce the chance of different positions having the same encoding\n",
    "\n",
    "<img src=\"./images/positional_encoding_s2.png\">\n",
    "\n",
    "- The sine and cosine functions have values in [-1, 1], which keeps the values of the positional encoding matrix in a normalized range.\n",
    "- As the sinusoid for each position is different, you have a unique way of encoding each position.\n",
    "- You have a way of measuring or quantifying the similarity between different positions, hence enabling you to encode the relative positions of words.\n",
    "\n",
    "NOTE: Can use with learnable parameters (but not have any paper confirm for better performance yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.0\n",
      "1: 0.8414709848078965\n",
      "2: 0.9092974268256817\n",
      "3: 0.1411200080598672\n",
      "4: -0.7568024953079282\n",
      "5: -0.9589242746631385\n",
      "0: 1.0\n",
      "1: 0.5403023058681398\n",
      "2: -0.4161468365471424\n",
      "3: -0.9899924966004454\n",
      "4: -0.6536436208636119\n",
      "5: 0.28366218546322625\n",
      "[[ 0.          1.          0.          1.        ]\n",
      " [ 0.84147098  0.54030231  0.00999983  0.99995   ]\n",
      " [ 0.90929743 -0.41614684  0.01999867  0.99980001]\n",
      " [ 0.14112001 -0.9899925   0.0299955   0.99955003]\n",
      " [-0.7568025  -0.65364362  0.03998933  0.99920011]\n",
      " [-0.95892427  0.28366219  0.04997917  0.99875026]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def gen_pe(max_length, d_model, n):\n",
    "\n",
    "    # generate an empty matrix for the positional encodings (pe)\n",
    "    pe = np.zeros(max_length*d_model).reshape(max_length, d_model) \n",
    "\n",
    "    # for each position\n",
    "    for k in np.arange(max_length):\n",
    "\n",
    "        # for each dimension\n",
    "        for i in np.arange(d_model//2):\n",
    "\n",
    "            # calculate the internal value for sin and cos\n",
    "            theta = k / (n ** ((2*i)/d_model))       \n",
    "\n",
    "            # even dims: sin   \n",
    "            pe[k, 2*i] = math.sin(theta) \n",
    "\n",
    "            # odd dims: cos               \n",
    "            pe[k, 2*i+1] = math.cos(theta)\n",
    "\n",
    "    return pe\n",
    "    \n",
    "for i in range(6):\n",
    "    s_result = math.sin(i)\n",
    "    print(f'{i}: {s_result}')\n",
    "\n",
    "for i in range(6):\n",
    "    c_result = math.cos(i)\n",
    "    print(f'{i}: {c_result}')\n",
    "\n",
    "# Used these vectors to added with input embedded vectors\n",
    "print(gen_pe(6, 4, 10000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
