{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attension (Attension Mechanism)\n",
    "\n",
    "REF: https://www.youtube.com/watch?v=QCJQG4DuHT0&ab_channel=CodeEmporium\n",
    "\n",
    "- Self-attention is a technique in machine learning that allows models to weigh the importance of different parts of an input sequence.\n",
    "- Self-Attension is solve the problem of RNN (Recurrent Neural Network) in case RNN is feed in model one by one word and can remember the weight of the last word that just feeded to model.\n",
    "\n",
    "INPUT: Word Vector\n",
    "OUTPUT: Word Vector with Self Attension included\n",
    "PROCESS:\n",
    "\n",
    "- Use Recurrent Neural Network\n",
    "- This process is come after Positional Encoding to fine-grained more context\n",
    "- Parameters\n",
    "    - L: length of words sequence\n",
    "    - d_k (Key): user's define\n",
    "    - d_v (Value): user's define\n",
    "- Equation of weight calculation (we will keep this for real learning later in ANN)\n",
    "\n",
    "For a single head:\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$ \n",
    "\n",
    "    - q, k, v:\n",
    "        - q: input x weight_q\n",
    "        - k: input x weight_k\n",
    "        - v: input x weight_v\n",
    "        - each q, k, v comes from Linear Transform Function (Q = W_Q * X + b_Q) | where X is input_batch\n",
    "    - np.matmul(q, k.T): Matrix multiply to get every word relationship (L x L)\n",
    "        - T: Transpose\n",
    "    - sqrt(dk): as denominator, Divind this to minimize variant and stabilize the vector\n",
    "    - V (Value): input vector\n",
    "    - new V: output after calculate self-attension\n",
    "    - M (Masking):\n",
    "```python\n",
    "mask = np.tril(np.ones((L, L)))\n",
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0\n",
    "```\n",
    "        - Ensure to not get future word while generating new word (prevent model to cheat)\n",
    "        - `required in decoder` but not require in encoder\n",
    "        - Triangular Matrix of 1: to let input focus only current and the past word (1) not future words (0) when multiply\n",
    "            - convert 1 to 0 and 0 to -inf: same concept but to support add (+) matrix and Softmax later\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attension\n",
    "\n",
    "REF: https://www.youtube.com/watch?v=HQn1QKQYXVg&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4&index=2&ab_channel=CodeEmporium\n",
    "\n",
    "- Allow Self-Attension to run in parallel\n",
    "- Separate the weight in each focus self-attension head - lead to better result\n",
    "- transpose each vector and send to each attension unit (head)\n",
    "\n",
    "##### How\n",
    "\n",
    "<img src=\"./images/multi_head_attension_s1.png\">\n",
    "\n",
    "- Convert each word to vector\n",
    "    - e.g. 512x1\n",
    "- Break down into 3 component vector (each have 512x1)\n",
    "    - Query: What am I looking for\n",
    "    - Key: What I can offer\n",
    "    - Value: What I ACTUALLY offer\n",
    "- Define attension heads number (e.g. 8) and break down each QKV into 8 piece (each 512 / 8 = 64 | 64 x 1)\n",
    "- All heads feed input to `Attension Unit`\n",
    "- Generate `Attension Matrix` for each head.\n",
    "    - 4x4 matrix (input has 4 words)\n",
    "    - Value is 0-1 (added up to 1) as probabilistic\n",
    "    - To generate contextual awareness vector\n",
    "- send to softmax function to normalized output in 0 - 1\n",
    "- concatenate every head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
