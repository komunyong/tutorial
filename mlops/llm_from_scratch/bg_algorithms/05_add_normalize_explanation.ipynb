{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add & Normalize Layer\n",
    "\n",
    "- (activation function) normalization normalize vector to close to zero\n",
    "    - easier to learn because value is normalized (standard deviation close to 1)\n",
    "    - faster and stable training to get to optimaal position\n",
    "    - more trackable\n",
    "- Equation:\n",
    "    - change equation from linear to use mean and standard deviation from whole layer (not just specific node)\n",
    "    - add gamma and beta as learnable parameters\n",
    "- ensure for every neuron in neural network are normalized\n",
    "- take input from multi-head attension and residual connection (before multi-head attension layer)\n",
    "- Residual connection: \n",
    "    - Just ADD!\n",
    "    - prevent \"vanishing gradient\" by use input to add directly to output after trained (duplicate input to skip training)\n",
    "    - Add & Norm layer that get `stronger information siignal`\n",
    "    - allow the input to a layer to bypass the layer’s operations and be added directly to the layer’s output\n",
    "    - To stabilize and improve the training of deep neural networks.\n",
    "    - to prevent vanishing gradient while back propagation, loss become 0 and stop learning\n",
    "    - The layers operation can be anything like Linear Transformations, Non-Linear Transformations, Normalization, Dropout, Pooling, etc.\n",
    "    - https://molgorithm.medium.com/what-is-add-norm-as-soon-as-possible-178fc0836381\n",
    "    - prevent vanishing gradient\n",
    "- Normalization\n",
    "    - normalizes the output across the “features.” to prevent Covariant Shift: “Internal Covariate Shift is the change in the distribution of network activations due to the change in network parameters during training”\n",
    "    - layer normalization (not batch norm): layer norm use 2 learnable params (gamme and beta learned after backpropagation)\n",
    "        - prevent \"restrictive data scales\": By learning these parameters, the model avoids the restriction of having all values between 0–1. Instead, it introduces fluctuations when necessary, allowing for a more dynamic and adaptive learning process. This enhances the model’s ability to capture and represent complex patterns in the data.  \n",
    "\n",
    "\n",
    "### Linear: concatenate all heads across layer\n",
    "### ReLu Activation Function: allow network to understand better in more complex \n",
    "### Dropout: random turn-off specific node in neuron to prevent network to remember specific pattern\n",
    "\n",
    "## Linear (training): to convert result back to dimensions\n",
    "\n",
    "### Add & Norm again to scopre down the result\n",
    "\n",
    "Drop out: random turn off the node in network to \"prevent the network to learned from memorize\" (called Regularization)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
