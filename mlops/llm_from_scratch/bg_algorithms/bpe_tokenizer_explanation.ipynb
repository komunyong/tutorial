{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE (Byte Pair Encoding)\n",
    "- Subword tokenizer\n",
    "- good with unknown word (OOV - Out of Vocabularies)\n",
    "\n",
    "- INPUT: Text Corpus\n",
    "- OUTPUT:\n",
    "    - Sub words Vocabulary (unique)\n",
    "    - Tokenized Text\n",
    "### REF:\n",
    "- https://www.youtube.com/watch?v=HEikzVL-lZU&ab_channel=HuggingFace\n",
    "- https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "- Get text corpus (full text)\n",
    "- Split by words\n",
    "- create set of words with frequency\n",
    "\n",
    "<img src=\"images/tokenizer_bpe_s1.png\">\n",
    "\n",
    "<img src=\"images/tokenizer_bpe_s2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "- create vocab with every single characters\n",
    "- Split each word into character based\n",
    "- Scan with each 2 characters to create 'pairs frequencies'\n",
    "\n",
    "<img src=\"images/tokenizer_bpe_s3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "- create vocab with every vocab (first round use every single character)\n",
    "- Split each word into character based\n",
    "- Scan with each 2 characters to of each words to create 'pairs frequencies'\n",
    "\n",
    "<img src=\"images/tokenizer_bpe_s3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "- Choose most occurence from 'pair frequencies list' to merge into new vocab\n",
    "- Add new vocab back to original vocab\n",
    "- Redo Step 2 and Step 3 again until reach vocab_limit (hyper parameter)\n",
    "- NOTE: After done training process we will get\n",
    "    - Sub words Vocabulary (unique)\n",
    "    - Tokenized Text (pair of {token-id: token-subword}) - to convert with encode and decode for machine understandable\n",
    "- NOTE: Mostly will use BPE as pre-trained algorithm\n",
    "\n",
    "<img src=\"images/tokenizer_bpe_s4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Testing Word\n",
    "- INPUT: Get new text as input\n",
    "- \n",
    "- Scan through 'vocab list'\n",
    "- Continue merge followed by vocab list until it cannot merge anymore\n",
    "- add to new vocab\n",
    "\n",
    "NOTE: We can use pre-trained BPE tokenizer (already have vocab list) from python `tiktoken` lib\n",
    "\n",
    "<img src=\"images/tokenizer_bpe_s5_test.png\">\n",
    "<img src=\"images/tokenizer_bpe_s6_test.png\">\n",
    "<img src=\"images/tokenizer_bpe_s7_test.png\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
